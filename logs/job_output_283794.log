cpu-bind=MASK - gpu19-1.drl, task  0  0 [2061817]: mask 0x1 set
Fri May 10 10:33:25 PM EDT 2024
Running my job on gpu19-1.drl
/tmp/home/amini/.local/lib/python3.10/site-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (3.9.5) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.
  warnings.warn(
/ - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / SETTING DEVICE AS cuda
Opening dataset in read-only mode as you don't have write permissions.
This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/activeloop/wiki-art

- \ | / - \ | / - \ | / - \ | / - \ hub://activeloop/wiki-art loaded successfully.

/tmp/home/amini/.local/lib/python3.10/site-packages/deeplake/integrations/pytorch/common.py:137: UserWarning: Decode method for tensors ['images'] is defaulting to numpy. Please consider specifying a decode_method in .pytorch() that maximizes the data preprocessing speed based on your transformation.
  warnings.warn(
Please wait, filling up the shuffle buffer with samples.:   0%|          | 0.00/2.00G [00:00<?, ?B/s]Please wait, filling up the shuffle buffer with samples.:   0%|          | 5.00M/2.00G [00:01<13:33, 2.63MB/s]Please wait, filling up the shuffle buffer with samples.:   3%|▎         | 64.0M/2.00G [00:02<00:55, 37.4MB/s]Please wait, filling up the shuffle buffer with samples.:   5%|▍         | 97.8M/2.00G [00:04<01:28, 23.0MB/s]Please wait, filling up the shuffle buffer with samples.:  13%|█▎        | 263M/2.00G [00:05<00:30, 60.6MB/s] Please wait, filling up the shuffle buffer with samples.:  24%|██▎       | 486M/2.00G [00:06<00:11, 142MB/s] Please wait, filling up the shuffle buffer with samples.:  30%|██▉       | 608M/2.00G [00:06<00:09, 165MB/s]Please wait, filling up the shuffle buffer with samples.:  43%|████▎     | 872M/2.00G [00:06<00:04, 302MB/s]Please wait, filling up the shuffle buffer with samples.:  50%|████▉     | 0.99G/2.00G [00:09<00:07, 151MB/s]Please wait, filling up the shuffle buffer with samples.:  70%|██████▉   | 1.40G/2.00G [00:09<00:02, 303MB/s]Please wait, filling up the shuffle buffer with samples.:  86%|████████▌ | 1.71G/2.00G [00:09<00:00, 451MB/s]Please wait, filling up the shuffle buffer with samples.:  94%|█████████▍| 1.88G/2.00G [00:13<00:00, 151MB/s]Please wait, filling up the shuffle buffer with samples.:  99%|█████████▉| 1.99G/2.00G [00:13<00:00, 175MB/s]Please wait, filling up the shuffle buffer with samples.: 100%|█████████▉| 2.00G/2.00G [00:13<00:00, 159MB/s]
/tmp/home/amini/.local/lib/python3.10/site-packages/torch/utils/_device.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return func(*args, **kwargs)
/tmp/home/amini/.local/lib/python3.10/site-packages/torch/utils/_device.py:78: UserWarning: Using a target size (torch.Size([1, 64, 64])) that is different to the input size (torch.Size([32, 64, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return func(*args, **kwargs)
/tmp/home/amini/.local/lib/python3.10/site-packages/torch/utils/_device.py:78: UserWarning: Using a target size (torch.Size([1, 128, 128])) that is different to the input size (torch.Size([32, 128, 128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return func(*args, **kwargs)
/tmp/home/amini/.local/lib/python3.10/site-packages/torch/utils/_device.py:78: UserWarning: Using a target size (torch.Size([32, 128, 128, 128])) that is different to the input size (torch.Size([1, 128, 128, 128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return func(*args, **kwargs)
/tmp/home/amini/.local/lib/python3.10/site-packages/torch/utils/_device.py:78: UserWarning: Using a target size (torch.Size([1, 256, 256])) that is different to the input size (torch.Size([32, 256, 256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return func(*args, **kwargs)
/tmp/home/amini/.local/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with an OutOfMemoryError: CUDA out of memory. Tried to allocate 1.13 GiB. GPU  (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:924.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/tmp/home/amini/.local/lib/python3.10/site-packages/torch/utils/_device.py:78: UserWarning: Plan failed with an OutOfMemoryError: CUDA out of memory. Tried to allocate 1.13 GiB. GPU  (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:924.)
  return func(*args, **kwargs)
/tmp/home/amini/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/tmp/home/amini/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/tmp/home/amini/.local/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with an OutOfMemoryError: CUDA out of memory. Tried to allocate 438.00 MiB. GPU  (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:924.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/tmp/home/amini/.local/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with an OutOfMemoryError: CUDA out of memory. Tried to allocate 866.00 MiB. GPU  (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:924.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Shuffle buffer filling is complete.
Building the style transfer model..
Optimizing..
0
20
40
run [50]:
Style Loss : 2.132188 Content Loss: 3.888123

60
80
run [100]:
Style Loss : 1.762955 Content Loss: 3.792062

100
120
140
run [150]:
Style Loss : 1.744920 Content Loss: 4.121816

160
180
run [200]:
Style Loss : 2.705007 Content Loss: 6.269286

200
220
240
run [250]:
Style Loss : 4.992435 Content Loss: 6.298448

260
280
run [300]:
Style Loss : 2.238203 Content Loss: 5.038579

300
Optimizing..
run [25]:
Loss : 0.020118, Floss: 0.020115, Mloss 0.000002

run [50]:
Loss : 0.006986, Floss: 0.006983, Mloss 0.000004

run [75]:
Loss : 0.003852, Floss: 0.003848, Mloss 0.000005

run [100]:
Loss : 0.002466, Floss: 0.002461, Mloss 0.000005

run [125]:
Loss : 0.001732, Floss: 0.001726, Mloss 0.000006

run [150]:
Loss : 0.001284, Floss: 0.001278, Mloss 0.000007

run [175]:
Loss : 0.000995, Floss: 0.000988, Mloss 0.000007

run [200]:
Loss : 0.000797, Floss: 0.000790, Mloss 0.000008

run [225]:
Loss : 0.000656, Floss: 0.000648, Mloss 0.000008

run [250]:
Loss : 0.000545, Floss: 0.000537, Mloss 0.000008

run [275]:
Loss : 0.000465, Floss: 0.000456, Mloss 0.000009

run [300]:
Loss : 0.000403, Floss: 0.000394, Mloss 0.000009

run [325]:
Loss : 0.000354, Floss: 0.000345, Mloss 0.000009

run [350]:
Loss : 0.000313, Floss: 0.000303, Mloss 0.000010

run [375]:
Loss : 0.000279, Floss: 0.000269, Mloss 0.000010

run [400]:
Loss : 0.000250, Floss: 0.000240, Mloss 0.000010

(256, 256, 3)
(256, 256, 3)
(256, 256, 3)
(256, 256, 3)
(256, 256, 3)
(256, 256, 3)
(256, 256, 3)
(256, 256, 3)
(256, 256, 3)
(256, 256, 3)
(256, 256, 3)
(256, 256, 3)
(256, 256, 3)
(256, 256, 3)
(256, 256, 3)
(256, 256, 3)
(256, 256, 3)
(256, 256, 3)
(256, 256, 3)
(256, 256, 3)
(256, 256, 3)
(256, 256, 3)
(256, 256, 3)
(256, 256, 3)
(256, 256, 3)
(256, 256, 3)
(256, 256, 3)
(256, 256, 3)
(256, 256, 3)
(256, 256, 3)
(256, 256, 3)
(256, 256, 3)
Building the style transfer model..
Optimizing..
0
20
40
run [50]:
Style Loss : 1.992674 Content Loss: 3.394614

60
80
Traceback (most recent call last):
  File "/data/scratch/amini/sadhana/2024/crypto-final-project/src/data_generation.py", line 350, in <module>
    output = run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,
  File "/data/scratch/amini/sadhana/2024/crypto-final-project/src/data_generation.py", line 239, in run_style_transfer
    optimizer.step(closure)
  File "/tmp/home/amini/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 391, in wrapper
    out = func(*args, **kwargs)
  File "/tmp/home/amini/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/tmp/home/amini/.local/lib/python3.10/site-packages/torch/optim/lbfgs.py", line 445, in step
    loss = float(closure())
  File "/tmp/home/amini/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/data/scratch/amini/sadhana/2024/crypto-final-project/src/data_generation.py", line 228, in closure
    loss.backward()
  File "/tmp/home/amini/.local/lib/python3.10/site-packages/torch/_tensor.py", line 516, in backward
    return handle_torch_function(
  File "/tmp/home/amini/.local/lib/python3.10/site-packages/torch/overrides.py", line 1619, in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
  File "/tmp/home/amini/.local/lib/python3.10/site-packages/torch/utils/_device.py", line 78, in __torch_function__
    return func(*args, **kwargs)
  File "/tmp/home/amini/.local/lib/python3.10/site-packages/torch/_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "/tmp/home/amini/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/tmp/home/amini/.local/lib/python3.10/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 
